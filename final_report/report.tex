\documentclass{article}
% \usepackage[margin=1.5in, includeheadfoot]{geometry}
\usepackage{biblatex-chicago}
\usepackage{fancyhdr}
\usepackage[fleqn]{amsmath}
\usepackage{amssymb, amsthm}
\usepackage{microtype}
\usepackage[shortlabels]{enumitem}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage[]{float}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage{fullpage}
\graphicspath{ {./images/} }

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    % backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    % numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\linespread{1.2}

\author{Gus Beringer}
\title{Capstone Project Draft}

\date{\today}
\newtheorem{theorem}{Theorem}
\def\D{\mathbb{D}}
\def\I{\mathbb{I}}
\def\N{\mathbb{N}}
\def\R{\mathbb{R}}
\def\T{\mathbb{T}}
\def\U{\mathbb{U}}
\def\Z{\mathbb{Z}}
\def\vep{{\varepsilon}}



\begin{document}
\maketitle
\tableofcontents

\section{Introduction}

Every Wikipedia article contains links to other articles. If the article references a concept that is also another Wikipedia article, the text can link to that article.
The Wikipedia hyperlink graph is the graph with every article as a node. The edges between the nodes are the links between the articles.

It is observable that the Wikipedia hyperlink graph is a small world network.
A small-world network is a graph where each node is connected to few nodes, but can reach all other nodes within a few steps.

An undirected graph is connected when there is a path between any two nodes in the graph.
If there is a path between any two nodes in a directed graph, it is a strongly connected.
If a directed graph has an undirected graph that is connected, it is weakly connected.

The adjacency matrix is defined as follows,
\begin{equation*}
    A_{i, j} = \begin{cases}
        1 & \textrm{an edge exists between $i$ and $j$} \\
        0 & \textrm{otherwise}
    \end{cases}.
\end{equation*}


\section{Sparse Adjacency Matrices}

The Wikipedia hyperlink graph has 6,102,910 nodes, which creates challenges for representing the graph in a program.
If we create an entry for every number in the adjacency matrix of a graph with $n$ nodes, the program requires $n^2$ space in total.  
For the wikipedia hyperlink graph, that's over $3.7 * 10^{13}$ entries.
However, most of the numbers in the adjacency matrix are zero, so can store the matrix more efficiently by only keeping track of non-zero entries.

The compressed sparse row (CSR) format is the most popular sparse matrix format.
We outline the Python package \textbf{scipy}'s implementation of the CSR sparse matrix.
There are three arrays that are used to represent the matrix, \textbf{data}, \textbf{indices}, and \textbf{indptr}.
\autocite{scipy_doc}
The \textbf{data} array contains the values for the non-zero elements in the order they appear in the matrix row by row.
The \textbf{indices} array contains the index of the column for each non-zero element.
The \textbf{indptr} array maps the matrix values to the rows in the matrix.
For an $m\times n$ matrix, the \textbf{indptr} array contains $m+1$ elements, where the first entry is always 0.

To extract the values of an arbitrary row $i$, we set $s = \textbf{indptr}[i]$ and $t = \textbf{indptr}[i+1]$. Then, the values of the items at row $i$ are $\textbf{data}[s:t]$ with columns corresponding to the array $\textbf{indices}[s:t]$. 

We provide the following example of the format with the following matrix,
\begin{equation*}
    \begin{pmatrix}
        52 & 0  & 0  & 0  \\
        0  & 0  & 43 & 0  \\
        67 & 0  & 0  & 81  \\
        0  & 20 & 0  & 0  \\
    \end{pmatrix}.
\end{equation*}
Reading row by row we find that, \textbf{data} = $[52, 43, 67, 81, 20]$, \textbf{indices} = $[0, 2, 0, 3, 1]$, and \textbf{indptr} = $[0, 1, 2, 4, 5]$. The number $52$ appears first in the matrix in the 0th column, so \textbf{data}$[0] = 52$ and \textbf{indices}$[0] = 0$.

In this example, the gains of using a sparse matrix are minimal. Instead of 16 entries, the matrix is represented with 15 entries. However, when there are far more zeroes than non-zero elements, the sparse format is efficient.


\section{Degree Distribution}

The degree of a node in an undirected graph is the number of edges connecting that node. For a directed graph we distinguish between the edges leaving and entering a node. 
The outdegree of a node is the number of edges that leave from that node.
The indegree of a node in a directed graph is the number of edges that enter that node from another node.

We define the indegree $d_{in}$ and outdegree $d_{out}$ mathematically:
Given a graph with $n$ nodes,
\begin{align*}
    d_{out} (v) & = \sum^n_{i=0} A_{v, i} \\
    d_{in} (v) & = \sum^n_{j=0} A_{j, i}. 
\end{align*}

We describe the distribution of nodes in the Wikipedia hyperlink graph.
The indegree distribution has a smooth exponential descent, while the outdegree distribution grows sharply before descending. 
Most of the network has a small indegree. We find that 93.84\% of the network has a indegree smaller than 300. However, there are still plenty of nodes with large indegree. We find 1,468 of nodes in the network with indegree larger than 10,000.

The outdegree distribution has a much smaller maximum of 11,897 nodes. This is because the outdegree is limited by the size of the article, but the indegree is only limited by the size of the graph.


\begin{figure}[H]
    \centering
    \caption[]{Degree Statistics}
    \begin{tabular}{lrrrrr}
        \toprule
        & Q1 & Median & Mean & Q3 & Maximum\\
        \midrule
        Outdegree & 4 & 19 & 91.46176 & 81 & 1,305,902\\
        Indegree & 17 & 39 & 91.46176 & 107 & 11,897\\
        \bottomrule
    \end{tabular}
\end{figure}

\begin{figure}[H]
    \centering
    \parbox{6cm}{
    \includegraphics[width=6cm]{in_degree_dist}
    \caption{Indegree Distribution}
    \label{fig:2figsA}}
    \qquad
    \begin{minipage}{6cm}
    \includegraphics[width=6cm]{out_degree_dist}
    \caption{Outdegree Distribution}
    \label{fig:2figsB}
    \end{minipage}
\end{figure}


\section{Centrality}

There are millions of nodes in the Wikipedia network, but not all of them are important.
The centrality measures provide methods of ranking nodes based on different measures of importance.
We evaluate and compare three different centrality metrics in the Wikipedia hyperlink graph.

\subsection{Degree Centrality}

Degree centrality is one of the simplest forms of centrality. The centrality of a node is simply the indegree or outdegree.
To compute the indegree and outdegree we simply sum the rows or columns of the adjacency matrix.

\subsubsection{Results}
\begin{figure}[H]
    \centering
    \caption{Highest Indegree}
    \begin{tabular}{llr}
        \toprule
        Article ID & Title & In Degree\\
        \midrule
        14919 & International\_Standard\_Book\_Number & 1305902\\
        48361 & Geographic\_coordinate\_system & 1151082\\
        25175562 & Virtual\_International\_Authority\_File & 904960\\
        18852926 & International\_Standard\_Name\_Identifier & 525539\\
        422994 & Digital\_object\_identifier & 471024\\
        \addlinespace
        3434750 & United\_States & 440874\\
        35412202 & Wikidata & 438109\\
        30463 & Taxonomy\_(biology) & 432978\\
        23538754 & Wayback\_Machine & 411661\\
        30890 & Time\_zone & 406023\\
        \addlinespace
        2987862 & Global\_Biodiversity\_Information\_Facility & 388044\\
        2855554 & IMDb & 339971\\
        39736 & Binomial\_nomenclature & 334349\\
        11039790 & Animal & 332711\\
        8439 & Diacritic & 322697\\
        \bottomrule
    \end{tabular}
\end{figure}
    
There are several pages in the Wikipedia network with excessive indegree compared to their outdegree. 
The article \textbf{International\_Standard\_Book\_Number} has an indegree of 1,322,167, but only an outdegree of 559. In fact, 20\% of all nodes in the graph have an edge connecting towards \textbf{International\_Standard\_Book\_Number}.

This is because of the automatic linking from references.
Any article with a citation that has an ISBN identifier will link to the page \textbf{International\_Standard\_Book\_Number}.
For similar reasons, the articles \textbf{Wayback\_Machine} and \textbf{OCLC} all have all have excessive indegree compared to their outdegree.

% TODO: OUTDEGREE

The outdegree results are even less useful. Instead of identifying the most important nodes, the highest outdegree only corresponds to the largest list.

\subsection{Closeness Centrality}

The motivation for closeness centrality is to identify the nodes that are closest to all other nodes in the network.

We are given a directed graph $G(V,E)$ with $n$ nodes and $m$ edges.
A directed graph is strongly connected if there is path between any two nodes. We assume that $G$ is strongly connected.
The distance $d(u, v)$ between nodes $u$ and $v$ is the shortest possible path between the nodes.
Since $G$ is strongly connected, $d(u, v)$ always exists.


% Given a graph $G(V, E)$, the distance $d(u, v)$ between  nodes 
Eppstein\autocite{eppstein} defines closeness centrality $c_v$ of a node $v$ as,
\begin{equation*}
    c_v = \frac{n-1}{\sum_{u \in V}d(u,v)}.
\end{equation*} 
The numerator $n-1$ normalizes the measure, making it comparable across different graphs.

% We define the following terms necessary for finding closeness centrality.
% The closeness centrality measure is an extension of the all pairs shortest path problem. That 

The single source shortest path (SSSP) problem involves finding the distance from a single source node to all other nodes in the graph.
The all pairs shortest path (APSP) problem involves finding the shortest path between all possible pairs of nodes in the graph.

To find the exact closeness centrality for all nodes in the network, we must solve the all pairs shortest path problem. However, for large networks this is computationally intensive. 
Eppstein provides a near-linear time approximation for centrality when the diameter is $O(\log n)$. That is, when the graph is a small world network. Since the Wikipedia hyperlink graph is a small world network, this algorithm is a good fit.

Then, Eppstein\autocite{eppstein} presents the following algorithm for computing closeness centrality,
\begin{enumerate}[1.]
    \item 
    Let k be the number of iterations needed to obtain the desired error bound.

    \item
    In iteration $i$, pick node $v_i$ uniformly at random from $G$ and solve the single source shortest path problem with $v_i$ as the source.

    \item 
    Let
    \begin{equation*}
        \hat{c}_ua = \frac{1}{\sum^k_{i=1} \frac{n d(v_i, u)}{k(n-1)}}
    \end{equation*}
    be the centrality estimator for node $u$.
\end{enumerate}

Since the graph is unweighted, we solve the SSSP problem in step two using a breadth first search. If the graph was unweighted, Dijkstra's algorithm could solve the SSSP in running time $O(E + V \log V)$.
\autocite{clrs} % TODO: verify.

\subsubsection{Results}

\begin{figure}[H]
    \caption[fig]{Closeness Centrality}
    \centering
    \begin{tabular}{rlr}
        \toprule
        Article ID & Title & Centrality \\
        \midrule
        14919 & International\_Standard\_Book\_Number & 0.5357142\\
        32927 & World\_War\_II & 0.5357142\\
        3434750 & United\_States & 0.5172413\\
        5843419 & France & 0.5172413\\
        23538754 & Wayback\_Machine & 0.5172413\\
        \addlinespace
        11867 & Germany & 0.4999999\\
        14532 & Italy & 0.4999999\\
        25391 & Russia & 0.4999999\\
        48361 & Geographic\_coordinate\_system & 0.4999999\\
        422994 & Digital\_object\_identifier & 0.4999999\\
        \addlinespace
        1057428 & Typographical\_error & 0.4999999\\
        26748 & Switzerland & 0.4838709\\
        31717 & United\_Kingdom & 0.4838709\\
        883885 & OCLC & 0.4838709\\
        5042916 & Canada & 0.4838709\\
    \bottomrule
\end{tabular}
\end{figure}

The World War II article is the only topic so far that is not a reference node or a country that has a high centrality.
The importance of World War II within Wikipedia is not surprising.
Most articles link to at least one country, and most countries link to the World War II article.
An unofficial Wikipedia game developed around 2010 called Clicks to Hitler, in which the participants try to reach the titular page as quickly as possible.
\autocite{cornell}
% https://blogs.cornell.edu/info2040/2019/10/23/the-significance-of-hitlers-wikipedia-page/

\subsection{Katz Centrality}

The Katz Centrality measure was introduced in 1953 by Leo Katz.
\autocite{katz1953}
In this measure of centrality, the weighted count of incoming paths to each node determines it's importance in the network. The attenuation factor $\alpha$ changes how quickly the weights decrease the importance of walks.

Katz Centrality is defined by,
\begin{equation*}
    C_{\textrm{Katz}}(i) = \sum_{j} (I_{ij} + \alpha A_{ij} + \alpha^2 A_{ij}^2 + \alpha^3 A_{ij}^3 + \dots).
\end{equation*}
\autocite{katz2011}

Then,
\begin{align*}
    C_{\textrm{Katz}} &= (I + \alpha A + \alpha^2 A^2 + \alpha^3 A^3 + \dots) \overrightarrow{1}\\
    &= \sum^\infty_{n=1} (\alpha^n A^n)\overrightarrow{1} \\ 
&= (I - \alpha A)^{-1} \overrightarrow{1} 
\end{align*}
% https://www.youtube.com/watch?v=DfV-pjRTlLg

To ensure that $C_{\textrm{Katz}}$ exists, we must choose an attenuation factor $\alpha$ such that the matrix $I - \alpha A$ is invertible. The matrix $I - \alpha A$ is non-invertible when,
\begin{align*}
    & \det (I - \alpha A) = 0 \\
    \implies \; & \det (A - \frac{1}{\alpha} I) = 0.
\end{align*}
This is simply the characteristic equation of $A$. Hence, we must find eigenvalues that satisfy the equation $\det(A - \lambda I)$, where $\lambda = \frac{1}{\alpha}$. Then, $\alpha = \frac{1}{\lambda}$.

Let $\lambda_1, \lambda_2, \dots, \lambda_n$ be the eigenvalues of $A$. We set $\alpha < \frac{1}{\max(\lambda_1, \dots, \lambda_n)}$.
Then, $\max(\lambda_1, \dots, \lambda_n) < \frac{1}{\alpha}$. Thus, $I - \alpha A$ is invertible.

% https://www.math.fsu.edu/~bertram/lectures/Centrality.pdf


With extremely large matrices the inverse is difficult and imprecise to compute directly, with an impracticable runtime of $O(n^3)$.
Therefore, we use power iteration algorithm to compute the measure instead.

We have,
\begin{align*}
    & C_{\textrm{Katz}} = (I - \alpha A)^{-1} \overrightarrow{1} \\
    \implies & (I - \alpha A) C_{\textrm{Katz}} = \overrightarrow{1} \\
    \implies & C_{\textrm{Katz}} = \alpha A C_{\textrm{Katz}} + \overrightarrow{1}
\end{align*}
We then define the following recurrence relation,
\begin{align*}
    x_0 & = \overrightarrow{1} \\
    x_k & = \alpha A x_{k+1} + \overrightarrow{1}
\end{align*} where the $x_k$ is an approximation of $C_{\textrm{Katz}}$. We then iterate until the norm of $x_{k} - x_{k-1}$ reaches a desired error.

\subsubsection{Results}
\begin{figure}[H]
    \centering
    \caption{Katz Centrality Results}
    \begin{tabular}{llr}
        \toprule
        Article ID & Title & Katz Centrality\\
        \midrule
        14919 & International\_Standard\_Book\_Number & 0.0864162\\
        422994 & Digital\_object\_identifier & 0.0549040\\
        48361 & Geographic\_coordinate\_system & 0.0511208\\
        25175562 & Virtual\_International\_Authority\_File & 0.0387391\\
    23538754 & Wayback\_Machine & 0.0342068\\
    \addlinespace
    234930 & International\_Standard\_Serial\_Number & 0.0341706\\
    30890 & Time\_zone & 0.0285691\\
    5843419 & France & 0.0271659\\
    3434750 & United\_States & 0.0267877\\
    35412202 & Wikidata & 0.0253573\\
    \addlinespace
    48455863 & Semantic\_Scholar & 0.0251241\\
    503009 & PubMed & 0.0229752\\
    47548 & Daylight\_saving\_time & 0.0222805\\
    30463 & Taxonomy\_(biology) & 0.0222407\\
    19828134 & Plant & 0.0219509\\
    \bottomrule
    \end{tabular}
\end{figure}

We see that high degree nodes dominate in Katz Centrality.
The top five nodes that are listed all have very high indegree.
The majority of the highest scoring nodes seem to be reference nodes, such as \textbf{International\_Standard\_Serial\_Number}, \textbf{Virtual\_International Authority\_File}, and \textbf{PubMed}.
The only exceptions are the two country articles, \textbf{United\_States} and \textbf{France}.

% \subsection{Comparing Measures}

% The Katz centrality and indegree centrality metrics both emphasize the importance of many citations from other articles. However, the centrality 


\section{Graph Radius \& Diameter}

The eccentricity of a node $v_i$ is the greatest distance between any other node $v$. That is, $e(v_i) = \max d(v_i, v)_{v \in V}$.

The radius of a graph is the minimum eccentricity of the entire graph, while the diameter of a graph is the maximum eccentricity.


Mathematically,
\begin{align*}
    r = \min e(v)_{v \in V} \\
    d = \max e(v)_{v \in V}
\end{align*}
where $r$ is the radius and $d$ is the diameter.

In the Wikipedia hyperlink graph the diameter can be conceptualized as given any article, how what is the maximum number of clicks to any other article in a shortest path. The radius can similarly be conceptualized as the minimum number of clicks to any other articles in a shortest path.


A naïve method of estimating the radius and the diameter is running multiple breadth first searches from random nodes and returning the minimum and maximum of the result. Boitmanis et. al presents a better approximation algorithm.
\autocite{boitmanis}
% https://sci-hub.se/https://dl.acm.org/doi/10.1007/11764298_9
Instead of starting the breadth first search from a uniformly random node, we start the breadth first search from the node furthest from the set of already processed nodes. This can be done in $O(n)$ time by keeping track of the results each breadth first search.

Using this search we can identify a tunnel of articles, from "Billboard Top Rock'n'Roll Hits: 1972" to "Billboard Top Hits: 1995". Where only a single path with 23 articles exists between the two articles.


\subsection{Results}

We estimate the radius and diameter of the 94.7\% of nodes that are strongly connected.
Using 20 iterations, the radius and diameter are both estimated at 33. That is, the eccentricity of the 20 tested nodes are all 33.
It is not clear if the estimated radius and diameter corresponds to the actual radius and diameter.
If the estimation is accurate, then from any article, the rest of the cluster is at most 33 clicks away.
This measure is robust to change against removing nodes.

Removing the first 10,000 nodes with the highest indegree results in no change to the estimated radius and diameter. 
This means that there are many different shortest paths between any two nodes in the network. When one node in the path is removed, there are still many paths available.

\section{Further Research}

One property of the hyperlink graph that isn't explored here is the order of the links. It is observable that repeatedly clicking the first link in nearly all articles will eventually lead to the Philosophy article. This is likely due to the definitional nature of the first paragraph in the article, that the first link in the article further abstracts the page.

% \subsection*{}

\printbibliography

\end{document}